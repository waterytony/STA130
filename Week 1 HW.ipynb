{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d31ceace",
   "metadata": {},
   "source": [
    "# PreLecture HW\n",
    "1. Pick one of the datasets from the ChatBot session(s) of the TUT demo (or from your own ChatBot session if you wish) and use the code produced through the ChatBot interactions to import the data and confirm that the dataset has missing values\n",
    "\n",
    "2. Start a new ChatBot session with an initial prompt introducing the dataset you're using and request help to determine how many columns and rows of data a  DataFrame has, and then\n",
    "\n",
    "    1. use code provided in your ChatBot session to print out the number of rows and columns of the dataset; and,\n",
    "    \n",
    "    2. write your own general definitions of the meaning of \"observations\" and \"variables\" based on asking the ChatBot to explain these terms in the context of your dataset\n",
    "    \n",
    "3. Ask the ChatBot how you can provide simple summaries of the columns in the dataset and use the suggested code to provide these summaries for your dataset\n",
    "\n",
    "4. If the dataset you're using has (a) non-numeric variables and (b) missing values in numeric variables, explain (perhaps using help from a ChatBot if needed) the discrepancies between size of the dataset given by  and what is reported by  with respect to (a) the number of columns it analyzes and (b) the values it reports in the \"count\" column\n",
    "\n",
    "# PostLecture HW\n",
    "6. The  method provides the 'count', 'mean', 'std', 'min', '25%', '50%', '75%', and 'max' summary statistics for each variable it analyzes. Give the definitions (perhaps using help from the ChatBot if needed) of each of these summary statistics\n",
    "\n",
    "7. Missing data can be considered \"across rows\" or \"down columns\".  Consider how  or  should be applied to most efficiently use the available non-missing data in your dataset and briefly answer the following questions in your own words\n",
    "\n",
    "    1. Provide an example of a \"use case\" in which using df.dropna() might be peferred over using del df['col']\n",
    "    \n",
    "    2. Provide an example of \"the opposite use case\" in which using del df['col'] might be preferred over using df.dropna()\n",
    "    \n",
    "    3. Discuss why applying del df['col'] before df.dropna() when both are used together could be importan\n",
    "    \n",
    "    4. Remove all missing data from one of the datasets you're considering using some combination of del df['col'] and/or df.dropna() and give a justification for your approach, including a \"before and after\" report of the results of your approach for your dataset.\n",
    "    \n",
    "8. Give brief explanations in your own words for any requested answers to the questions below\n",
    "    1. Use your ChatBot session to understand what df.groupby(\"col1\")[\"col2\"].describe() does and then demonstrate and explain this using a different example from the \"titanic\" data set other than what the ChatBot automatically provide for you\n",
    "    \n",
    "    2. Assuming you've not yet removed missing values in the manner of question \"7\" above, df.describe() would have different values in the count value for different data columns depending on the missingness present in the original data.  Why do these capture something fundamentally different from the values in the count that result from doing something like df.groupby(\"col1\")[\"col2\"].describe()?\n",
    "    \n",
    "    3. Intentionally introduce the following errors into your code and report your opinion as to whether it's easier to (a) work in a ChatBot session to fix the errors, or (b) use google to search for and fix errors: first share the errors you get in the ChatBot session and see if you can work with ChatBot to troubleshoot and fix the coding errors, and then see if you think a google search for the error provides the necessary toubleshooting help more quickly than ChatGPT\n",
    "    \n",
    "        1. Forget to include import pandas as pd in your code <br> Use Kernel->Restart from the notebook menu to restart the jupyter notebook session unload imported libraries and start over so you can create this error <br><br> When python has an error, it sometimes provides a lot of \"stack trace\" output, but that's not usually very important for troubleshooting. For this problem for example, all you need to share with ChatGPT or search on google is \"NameError: name 'pd' is not defined\"<br><br>\n",
    "        \n",
    "        2. Mistype \"titanic.csv\" as \"titanics.csv\" <br> If ChatBot troubleshooting is based on downloading the file, just replace the whole url with \"titanics.csv\" and try to troubleshoot the subsequent FileNotFoundError: [Errno 2] No such file or directory: 'titanics.csv' (assuming the file is indeed not present) <br><br> Explore introducing typos into a couple other parts of the url and note the slightly different errors this produces<br><br>\n",
    "        \n",
    "        3. Try to use a dataframe before it's been assigned into the variable <br> You can simulate this by just misnaming the variable. For example, if you should write df.groupby(\"col1\")[\"col2\"].describe() based on how you loaded the data, then instead write DF.groupby(\"col1\")[\"col2\"].describe() <br><br> Make sure you've fixed your file name so that's not the error any more<br><br>\n",
    "        \n",
    "        4. Forget one of the parentheses somewhere the code <br> For example, if the code should be pd.read_csv(url) the change it to pd.read_csv(url<br><br>\n",
    "        \n",
    "        5. Mistype one of the names of the chained functions with the code <br> For example, try something like df.group_by(\"col1\")[\"col2\"].describe() and df.groupby(\"col1\")[\"col2\"].describle()<br><br>\n",
    "        \n",
    "        6. Use a column name that's not in your data for the groupby and column selection <br> For example, try capitalizing the columns for example replacing \"sex\" with \"Sex\" in titanic_df.groupby(\"sex\")[\"age\"].describe(), and then instead introducing the same error of \"age\"<br><br>\n",
    "            \n",
    "        7. Forget to put the column name as a string in quotes for the groupby and column selection, and see if the ChatBot and google are still as helpful as they were for the previous question <br> For example, something like titanic_df.groupby(sex)[\"age\"].describe(), and then titanic_df.groupby(\"sex\")[age].describe()\n",
    "        \n",
    "9. Have you reviewed the course  and interacted with a ChatBot (or, if that wasn't sufficient, real people in the course piazza discussion board or TA office hours) to help you understand all the material in the tutorial and lecture that you didn't quite follow when you first saw it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b2128a",
   "metadata": {},
   "source": [
    "# ChatBot History/Log\n",
    "## NotebookLM\n",
    "### Summary: *\"Please provide a summary of our exchanges here so I can submit them as a record of our interactions as part of a homework assignment\"*\n",
    "Here is a summary of your previous exchanges, which you can use as a record of your interactions for your homework assignment:\n",
    "●\n",
    "You initially asked for a list of things to complete for Week 1 of STA130.\n",
    "●\n",
    "The response provided a breakdown of the Prelecture and Postlecture homework tasks.\n",
    "●\n",
    "It also included key concepts for Week 1, such as Data Summarization and Coding.\n",
    "●\n",
    "You then asked about the topics discussed in LEC01.\n",
    "●\n",
    "The response summarized the key points of LEC01 from the source material, including Data Summarization techniques, Functions, Arguments, Attributes, Methods, Chaining, Coercion, Statistics, and subsetting data.\n",
    "The interaction highlights the importance of understanding data summarization techniques and basic coding concepts covered in Week 1 of STA130. You can use this summary as part of your homework assignment, remembering to cite the sources appropriately as needed.\n",
    "### Prompt:\n",
    "what are the things I need to do for week 1\n",
    "\n",
    "### Response:\n",
    "Here are some things you need to do for week 1:\n",
    "●\n",
    "Complete the Prelecture Homework:\n",
    "○\n",
    "Confirm a dataset has missing values using code from a ChatBot session. You can use a dataset from the TUT demo, your own ChatBot session, or the Titanic dataset.\n",
    "○\n",
    "Start a new ChatBot session and determine the number of rows and columns in the dataset. Write your own definitions of \"observations\" and \"variables.\"\n",
    "○\n",
    "Ask the ChatBot how to provide simple summaries of the columns in the dataset. Create working examples of using  df.describe()  and  df['column'].value_counts()  for your dataset.\n",
    "○\n",
    "Explain the discrepancies in dataset size between  df.shape  and  df.describe()  for datasets with non-numeric variables and missing values in numeric variables.\n",
    "○\n",
    "Use your ChatBot session to understand the difference between an \"attribute\" and a \"method\" and provide your own paraphrasing summarization of that difference.\n",
    "●\n",
    "Complete the Postlecture Homework:\n",
    "○\n",
    "Define each of the summary statistics provided by the  df.describe()  method, and consider how it handles missing data.\n",
    "○\n",
    "Explain how to use  df.dropna()  and  del df['col'] to efficiently remove missing data, providing examples and justifications for different use cases.\n",
    "○\n",
    "Start a new ChatBot session and demonstrate and explain the use of  df.groupby(\"col1\")[\"col2\"].describe()  with an example from the Titanic dataset.\n",
    "○\n",
    "Compare the count values from  df.describe()  and  df.groupby(\"col1\")[\"col2\"].describe()  and explain the differences.\n",
    "○\n",
    "Intentionally introduce errors into your code and compare the ease of troubleshooting using a ChatBot versus Google search. Report your findings.\n",
    "○\n",
    "Review the course wiki and consult a ChatBot, Piazza, or TA office hours for any unclear tutorial or lecture material.\n",
    "●\n",
    "Recommended Additional Activities (Optional):\n",
    "○\n",
    "Consider ethical implications of data analysis, such as survivorship bias.\n",
    "○\n",
    "Familiarize yourself with the Canadian Social Connection Survey dataset for the course project.\n",
    "○\n",
    "Explore additional ChatBot interactions to enhance your understanding of statistics and coding.\n",
    "●\n",
    "Key Concepts for Week 1:\n",
    "○\n",
    "Data Summarization:  Learn to import libraries (like pandas), load data, count missing values, differentiate between observations (rows) and variables (columns), distinguish numeric and non-numeric data, remove missing data, and perform grouping and aggregation.\n",
    "○\n",
    "Coding:  Understand basic data types like tuples, lists, dictionaries, and arrays. Learn about loops, logical flow control with if-else statements and try-except blocks.\n",
    "Remember to document your ChatBot interactions and code within your Jupyter notebook and submit it via Quercus.\n",
    "\n",
    "### Prompt:\n",
    "what was talked about in the lec01\n",
    "\n",
    "### Response:\n",
    "Here's a summary of the topics discussed in LEC01, based on the provided source materials:\n",
    "●\n",
    "Data Summarization\n",
    ":  LEC01 introduced key concepts and techniques for summarizing data using Python libraries like pandas.  This included importing libraries, loading data using  pd.read_csv() with different arguments (like encoding), understanding data types (numeric, categorical, and object), handling missing values (df.isnull().sum()), and calculating basic statistics (mean, variance, standard deviation, min, max). The lecture also touched upon sorting data (.sort_values()), indexing (0-based indexing, df.iloc[], slicing with :), and subsetting data using boolean selection and logical conditionals (>=, >, <=, <, ==, !=, ~, &, |).\n",
    "●\n",
    "Functions, Arguments, and Attributes\n",
    ": The lecture differentiated between functions like pd.read_csv() and methods like .shape and .columns that belong to pandas DataFrame objects. It explained the concept of arguments that control the behavior of functions, such as inplace=\"ISO-8859-1\" for encoding in pd.read_csv(). The lecture also highlighted the use of attributes like .shape and .columns to access information about a DataFrame.\n",
    "●\n",
    "Methods, Chaining, and Coercion\n",
    ": LEC01 explained method chaining in pandas, where multiple methods are called sequentially on a DataFrame, such as df.isnull().sum(axis=1). It also discussed coercion, specifically how boolean values (True/False) are converted to integers (1/0) when using .sum()  after  .isna().\n",
    "●\n",
    "Statistics vs. Statistics\n",
    ": The lecture emphasized the distinction between statistics (numerical summaries of data, like the sample mean) and Statistics (the broader field of study). It provided formulas and pandas methods for calculating common statistics, including the sample mean (df['col'].mean()), sample variance (df['col'].var()), and sample standard deviation (df['col'].std()).\n",
    "●\n",
    "Summarizing Numeric and Categorical Data\n",
    ": LEC01 highlighted the different approaches for summarizing numeric and categorical data. For numeric data, it covered df.describe(), which provides statistics like count, mean, standard deviation, min, 25th percentile (Q1), median (50th percentile), 75th percentile (Q3), and max.  For categorical data, it introduced  df['categorical_column'].value_counts() to count the occurrences of each unique value (mode).\n",
    "●\n",
    "Subsetting Data\n",
    ":  The lecture reviewed different ways to subset data from a pandas DataFrame, including using 0-based row and column indexing (df[]), the  .iloc[] attribute for fully 0-based indexing and slicing, and boolean selection with logical conditionals using either the df[] or df.loc[] attributes.\n",
    "●\n",
    "Potential Additional Content (Time Permitting)\n",
    ": If time had allowed, LEC01 might have further demonstrated these data summarization concepts using the Pokemon dataset.  It could have also included more complex examples of method chaining, like  df.dropna().groupby('col1').describe().\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69bd9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
